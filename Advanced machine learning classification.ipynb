{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27b2357f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##**Data Science in Health Care. Advanced machine learning classification**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb590ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##**This will show us how to upload images, transform them, and determine the basic features that underlie diseases classification.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "696aaf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "##**Two different approaches to the classification of images (diseases) will be shown 1-Different classical methods and their comparison 2- Convolutional Neural Networks.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1761a4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Objectives\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95d237f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Download and transform images.\n",
    "##Create features of images.\n",
    "##Build different classification models.\n",
    "##Build CNN models.\n",
    "##Build a diagnosis based on X-ray photos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a5e0771",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mahotas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmahotas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmh\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pyplot \u001b[38;5;28;01mas\u001b[39;00m plt \n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mahotas'"
     ]
    }
   ],
   "source": [
    "import mahotas as mh\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt \n",
    "from glob import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "#Classifiers\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a0ab9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79699dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMM_SIZE = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86058ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##The function returns an array of tuples \\[image, class name].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5ce10ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(folder):\n",
    "    class_names = [f for f in os.listdir(folder) if not f.startswith('.')] # ctreate a list of SubFolders\n",
    "    data = []\n",
    "    print(class_names)\n",
    "    for t, f in enumerate(class_names):\n",
    "        images = glob(folder + \"/\" + f + \"/*\") # create a list of files\n",
    "        print(\"Downloading: \", f)\n",
    "        fig = plt.figure(figsize = (50,50)) \n",
    "        for im_n, im in enumerate(images):\n",
    "            plt.gray() # set grey colormap of images\n",
    "            image = mh.imread(im)\n",
    "            if len(image.shape) > 2:\n",
    "                image = mh.resize_to(image, [IMM_SIZE, IMM_SIZE, image.shape[2]]) # resize of RGB and png images\n",
    "            else:\n",
    "                image = mh.resize_to(image, [IMM_SIZE, IMM_SIZE]) # resize of grey images    \n",
    "            if len(image.shape) > 2:\n",
    "                image = mh.colors.rgb2grey(image[:,:,:3], dtype = np.uint8)  # change of colormap of images alpha chanel delete\n",
    "            plt.subplot(int(len(images)/5)+1,5,im_n+1) # create a table of images\n",
    "            plt.imshow(image)\n",
    "            data.append([image, f])\n",
    "        plt.show()\n",
    "\n",
    "    return np.array(data)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30231d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "##For training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f6793c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m d \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCovid19-dataset/train\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m train \u001b[38;5;241m=\u001b[39m \u001b[43mget_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m d \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCovid19-dataset/test\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m val \u001b[38;5;241m=\u001b[39m get_data(d)\n",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36mget_data\u001b[0;34m(folder)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_data\u001b[39m(folder):\n\u001b[0;32m----> 2\u001b[0m     class_names \u001b[38;5;241m=\u001b[39m [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mlistdir(folder) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)] \u001b[38;5;66;03m# ctreate a list of SubFolders\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     data \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(class_names)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "d = \"Covid19-dataset/train\"\n",
    "train = get_data(d)\n",
    "\n",
    "d = \"Covid19-dataset/test\"\n",
    "val = get_data(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7030d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train shape\", train.shape) # Size of the training DataSet\n",
    "print(\"Test shape\", val.shape) # Size of the test DataSet\n",
    "print(\"Image size\", train[0][0].shape) # Size of image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ea422ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "##the training DataSet consists of 251 images and the test one consists of 66 images. All the images are in grey 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e42c523f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a281093c",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "for i in train:\n",
    "    l.append(i[1])\n",
    "sns.set_style('darkgrid')\n",
    "sns.countplot(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e777abbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (5,5))\n",
    "plt.imshow(train[np.where(train[:,1] == 'Viral Pneumonia')[0][0]][0])\n",
    "plt.title('Viral Pneumonia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b140b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (5,5))\n",
    "plt.imshow(train[np.where(train[:,1] == 'Covid')[0][0]][0])\n",
    "plt.title('Covid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "469fc0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "##features creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135d3c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(data):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for image, label in data:\n",
    "        features.append(mh.features.haralick(image).ravel())\n",
    "        labels.append(label)\n",
    "    features = np.array(features)\n",
    "    labels = np.array(labels)\n",
    "    return (features, labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb6af9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train, labels_train = create_features(train)\n",
    "features_test, labels_test = create_features(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1337e852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing different classical classification methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf54c46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Pipeline([('preproc', StandardScaler()), ('classifier', LogisticRegression())])\n",
    "clf.fit(features_train, labels_train)\n",
    "scores_train = clf.score(features_train, labels_train)\n",
    "scores_test = clf.score(features_test, labels_test)\n",
    "print('Training DataSet accuracy: {: .1%}'.format(scores_train), 'Test DataSet accuracy: {: .1%}'.format(scores_test))\n",
    "plot_confusion_matrix(clf, features_test, labels_test)  \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43bafff",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"Logistic Regression\", \"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \"Gaussian Process\",\n",
    "         \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n",
    "         \"Naive Bayes\", \"QDA\"]\n",
    "\n",
    "classifiers = [\n",
    "    LogisticRegression(),\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "    SVC(gamma=2, C=1),\n",
    "    GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    MLPClassifier(alpha=1, max_iter=1000),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis()]\n",
    "scores_train = []\n",
    "scores_test = []\n",
    "for name, clf in zip(names, classifiers):\n",
    "    clf = Pipeline([('preproc', StandardScaler()), ('classifier', clf)])\n",
    "    clf.fit(features_train, labels_train)\n",
    "    score_train = clf.score(features_train, labels_train)\n",
    "    score_test = clf.score(features_test, labels_test)\n",
    "    scores_train.append(score_train)\n",
    "    scores_test.append(score_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23168c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Let's print the results as a table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4cde4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame(index = names)\n",
    "res['scores_train'] = scores_train\n",
    "res['scores_test'] = scores_test\n",
    "res.columns = ['Test','Train']\n",
    "res.index.name = \"Classifier accuracy\"\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dfa65689",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Let's compare the results on a plot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b30b53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(len(names))  # the label locations\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width/2, scores_train, width, label='Train')\n",
    "rects2 = ax.bar(x + width/2, scores_test, width, label='Test')\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Accuracy of classifiers')\n",
    "ax.set_xticks(x)\n",
    "plt.xticks(rotation = 90)\n",
    "ax.set_xticklabels(names)\n",
    "ax.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b4d409",
   "metadata": {},
   "outputs": [],
   "source": [
    "##now we are fitting Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa921f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout \n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d353d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "x_val = []\n",
    "y_val = []\n",
    "\n",
    "for feature, label in train:\n",
    "    x_train.append(feature)\n",
    "    y_train.append(label)\n",
    "\n",
    "for feature, label in val:\n",
    "    x_val.append(feature)\n",
    "    y_val.append(label)\n",
    "\n",
    "# Normalize the data\n",
    "x_train = np.array(x_train) / 255\n",
    "x_val = np.array(x_val) / 255\n",
    "\n",
    "# Reshaping input images\n",
    "x_train = x_train.reshape(-1, IMM_SIZE, IMM_SIZE, 1)\n",
    "x_val = x_val.reshape(-1, IMM_SIZE, IMM_SIZE, 1)\n",
    "\n",
    "# Creating a dictionary of clases\n",
    "lab = {}\n",
    "for i, l in enumerate(set(y_train)):\n",
    "    lab[l] = i\n",
    "\n",
    "\n",
    "y_train = np.array([lab[l] for l in y_train])\n",
    "y_val = np.array([lab[l] for l in y_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21a264e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of the input DataSet:\", x_train.shape)\n",
    "print(\"Shape of the output DataSet:\", y_train.shape)\n",
    "print(\"Dictionary of classes:\", lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc76fe96",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model defining\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd929fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32,1,padding=\"same\", activation=\"relu\", input_shape=(IMM_SIZE,IMM_SIZE,1)))\n",
    "model.add(MaxPool2D())\n",
    "\n",
    "model.add(Conv2D(32, 1, padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPool2D())\n",
    "\n",
    "model.add(Conv2D(64, 1, padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPool2D())\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128,activation=\"relu\"))\n",
    "model.add(Dense(3, activation=\"softmax\"))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7f086c",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(lr=0.000001)\n",
    "model.compile(optimizer = opt , loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) , metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cb0dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitting = False\n",
    "fitting_save = False\n",
    "epochs = 2000\n",
    "\n",
    "import pickle\n",
    "\n",
    "if fitting:\n",
    "    history = model.fit(x_train,y_train,epochs = epochs , validation_data = (x_val, y_val), shuffle = True)\n",
    "    if fitting_save:\n",
    "    # serialize model to JSON\n",
    "        model_json = model.to_json()\n",
    "        with open(\"model.json\", \"w\") as json_file:\n",
    "            json_file.write(model_json)\n",
    "        # serialize weights to HDF5\n",
    "        model.save_weights(\"model.h5\")\n",
    "        print(\"Saved model to disk\")\n",
    "        with open('history.pickle', 'wb') as f:\n",
    "            pickle.dump(history.history, f)\n",
    "        with open('lab.pickle', 'wb') as f:\n",
    "            pickle.dump(lab, f)\n",
    "# load model  \n",
    "from keras.models import model_from_json\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(loaded_model_json)\n",
    "# load weights into a new model\n",
    "model.load_weights(\"model.h5\")        \n",
    "with open('history.pickle', 'rb') as f:\n",
    "    history = pickle.load(f)\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ab74bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c70e263",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history['accuracy']\n",
    "val_acc = history['val_accuracy']\n",
    "loss = history['loss']\n",
    "val_loss = history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3e05c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "predictions = model.predict_classes(x_val)\n",
    "predictions = predictions.reshape(1,-1)[0]\n",
    "print(classification_report(y_val, predictions, target_names = lab.keys()))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = pd.DataFrame(confusion_matrix(y_val, predictions))\n",
    "cm.index = [\"Predicted \" + s for s in lab.keys()]\n",
    "cm.columns = [\"True  \" + s for s in lab.keys()]\n",
    "print(cm)\n",
    "\n",
    "sns.heatmap(confusion_matrix(y_val, predictions), annot=True, \n",
    "            xticklabels = list(lab.keys()), yticklabels = list(lab.keys()))\n",
    "plt.xlabel(\"True labels\")\n",
    "plt.ylabel(\"Predicted labels\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21052286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "z = model.predict_classes(x_train) == y_train\n",
    "scores_train = sum(z+0)/len(z)\n",
    "z = model.predict_classes(x_val) == y_val\n",
    "scores_test = sum(z+0)/len(z)\n",
    "print('Training DataSet accuracy: {: .1%}'.format(scores_train), 'Test DataSet accuracy: {: .1%}'.format(scores_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62693c35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python396jvsc74a57bd0b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
